#!/usr/bin/env python3
"""
ğŸš€ LocoMuJoCo Tutorial 4: Learning to Walk - Reinforcement Learning Basics
==========================================================================

This tutorial introduces the basic concepts of how robots learn to walk
through reinforcement learning - trial and error with rewards!

LEARNING OBJECTIVES:
ğŸ“š Understand what reinforcement learning is
ğŸ¯ See how reward functions guide robot behavior
ğŸ¤– Watch a robot gradually improve at walking
ğŸ§  Grasp the training process and why it works

No advanced ML knowledge required - just curiosity!
"""

import os
import time
import numpy as np
import jax
import jax.numpy as jnp
from loco_mujoco.task_factories import RLFactory, ImitationFactory, DefaultDatasetConf


def explain_reinforcement_learning():
    """ğŸ’¡ Explain RL concepts with analogies"""
    print("\nğŸ’¡ CONCEPT: What is Reinforcement Learning?")
    print("â”€" * 60)
    print("   Imagine teaching a child to ride a bike:")
    print("   â€¢ TRIAL: Child tries to balance and pedal")
    print("   â€¢ ERROR: Child wobbles or falls")  
    print("   â€¢ REWARD: 'Good job!' when they stay upright")
    print("   â€¢ LEARNING: Child gradually gets better")
    print("   ")
    print("   For robots, it's the same:")
    print("   â€¢ Robot tries different movements (ACTIONS)")
    print("   â€¢ Environment gives feedback (REWARDS)")
    print("   â€¢ Robot learns which actions work best")
    print("   â€¢ Eventually robot masters the skill!")
    print("â”€" * 60)


def demonstrate_reward_system():
    """ğŸ¯ Show how reward functions work"""
    print("\nğŸ’¡ CONCEPT: Reward Functions - Teaching Robots What's Good")
    print("â”€" * 60)
    print("   A reward function is like a teacher's grading system:")
    print("   ")
    print("   ğŸ† POSITIVE REWARDS (Good behavior):")
    print("   â€¢ +10 points for walking forward")
    print("   â€¢ +5 points for staying upright") 
    print("   â€¢ +2 points for smooth movements")
    print("   ")
    print("   ğŸ’¥ NEGATIVE REWARDS (Bad behavior):")
    print("   â€¢ -20 points for falling down")
    print("   â€¢ -5 points for moving backward")
    print("   â€¢ -1 point for jerky movements")
    print("   ")
    print("   ğŸ§  The robot learns to maximize total reward!")
    print("â”€" * 60)


def simulate_learning_process():
    """ğŸ“ Simulate the learning process conceptually"""
    print("\nğŸ“ STEP 1: Simulating the Learning Process")
    print("Let's see how a robot might learn to walk over time...")
    
    # Simulate learning progress
    episodes = [
        {"episode": 1, "reward": -150, "behavior": "Falls immediately", "lesson": "Random actions don't work"},
        {"episode": 10, "reward": -45, "behavior": "Stands for 3 seconds", "lesson": "Learning to balance"},
        {"episode": 50, "reward": 25, "behavior": "Takes 2 steps forward", "lesson": "Discovering walking motion"},
        {"episode": 100, "reward": 180, "behavior": "Walks 5 meters smoothly", "lesson": "Mastering coordination"},
        {"episode": 500, "reward": 450, "behavior": "Walks gracefully, turns", "lesson": "Expert-level performance"}
    ]
    
    print("\nğŸ“ˆ Learning Progress Over Time:")
    print("=" * 70)
    
    for ep in episodes:
        print(f"\nğŸ”„ Episode {ep['episode']:3d}: Reward = {ep['reward']:4d}")
        print(f"   ğŸ¤– Behavior: {ep['behavior']}")
        print(f"   ğŸ“ Lesson: {ep['lesson']}")
        time.sleep(1.5)
    
    print("\nğŸ’¡ Key Insight: The robot discovers successful strategies")
    print("   through thousands of trial-and-error attempts!")


def demonstrate_real_training_setup():
    """ğŸ”¬ Show actual RL training environment"""
    print(f"\nğŸ”¬ STEP 2: Real RL Training Environment")
    print("Now let's see an actual reinforcement learning setup...")
    
    try:
        print(f"\nğŸ“‹ Creating RL training environment...")
        
        # Create RL environment (no pre-defined motion data)
        env = RLFactory.make("UnitreeG1")
        
        print(f"âœ… RL Environment created!")
        print(f"ğŸ® Action space: {env.num_actions} motors to control")
        print(f"ğŸ” Observation space: {env.num_observations} sensor readings")
        
        print(f"\nğŸ’¡ CONCEPT: The Difference")
        print("â”€" * 60)
        print("   ğŸ¬ IMITATION Learning: Robot copies human demonstrations")  
        print("   ğŸ¯ REINFORCEMENT Learning: Robot discovers actions through rewards")
        print("   ")
        print("   RL is harder but more flexible - robots can learn")
        print("   movements that no human has ever demonstrated!")
        print("â”€" * 60)
        
        # Show what an untrained robot does
        print(f"\nğŸ¤– What an untrained robot does:")
        print("   Let's see random actions (before any learning)...")
        
        key = jax.random.PRNGKey(42)
        obs = env.reset(key)
        
        print(f"   ğŸ“Š Initial observation shape: {obs.shape}")
        print(f"   ğŸ² Taking random actions for 3 seconds...")
        
        total_reward = 0
        for step in range(90):  # 3 seconds at 30 FPS
            # Random action (untrained robot)
            action = np.random.uniform(-0.1, 0.1, env.num_actions) 
            
            result = env.step(action)
            if len(result) == 5:
                obs, reward, done, truncated, info = result
            else:
                obs, reward, done, info = result
                truncated = False
            
            total_reward += reward
            
            if done:
                print(f"   ğŸ’¥ Robot fell at step {step}!")
                break
        
        print(f"   ğŸ“Š Total reward: {total_reward:.2f}")
        print(f"   ğŸ’¡ Negative reward = robot needs to learn better actions!")
        
        del env
        
    except Exception as e:
        print(f"   âš ï¸  Could not create RL environment: {e}")
        print(f"   ğŸ’¡ This is normal - RL training requires special setup")


def compare_learning_approaches():
    """âš–ï¸ Compare different learning approaches"""
    print(f"\nâš–ï¸ STEP 3: Learning Approach Comparison")
    print("=" * 60)
    
    approaches = [
        {
            "name": "Imitation Learning", 
            "emoji": "ğŸ¬",
            "method": "Copy human demonstrations",
            "pros": "Fast, human-like movements",
            "cons": "Limited to demonstrated behaviors",
            "example": "Learning to dance by watching videos"
        },
        {
            "name": "Reinforcement Learning",
            "emoji": "ğŸ¯", 
            "method": "Trial-and-error with rewards",
            "pros": "Can discover novel solutions",
            "cons": "Slower, requires many trials", 
            "example": "Learning chess by playing millions of games"
        },
        {
            "name": "Hybrid Approach",
            "emoji": "ğŸš€",
            "method": "Start with imitation, improve with RL", 
            "pros": "Fast start + continued improvement",
            "cons": "More complex to implement",
            "example": "Learn basic walking, then optimize for speed"
        }
    ]
    
    for approach in approaches:
        print(f"\n{approach['emoji']} {approach['name'].upper()}")
        print(f"   ğŸ“‹ Method: {approach['method']}")
        print(f"   âœ… Pros: {approach['pros']}")  
        print(f"   âš ï¸  Cons: {approach['cons']}")
        print(f"   ğŸŒŸ Example: {approach['example']}")


def demonstrate_with_imitation_baseline():
    """ğŸ¬ Show how we currently achieve walking with imitation"""
    print(f"\nğŸ¬ STEP 4: Current Solution - Imitation Learning")
    print("While true RL training takes hours, let's see imitation learning...")
    
    try:
        print(f"\nğŸ“‹ Loading human walking data...")
        
        # Create imitation environment (using human demonstrations)
        env = ImitationFactory.make("UnitreeG1", 
                                  default_dataset_conf=DefaultDatasetConf(["walk"]))
        
        print(f"âœ… Imitation environment ready!")
        print(f"ğŸ“š This uses recorded human walking motions")
        
        print(f"\nğŸ¯ Quick demonstration: Robot walking (10 seconds)")
        print("   This shows what RL training aims to achieve...")
        
        env.play_trajectory(
            n_episodes=1,
            n_steps_per_episode=300,  # 10 seconds
            render=True
        )
        
        print(f"âœ… Beautiful walking motion achieved through imitation!")
        print(f"ğŸ¯ Goal: RL would learn to create this without human data")
        
        del env
        
    except Exception as e:
        print(f"   âš ï¸  Demo unavailable: {e}")


def main():
    """ğŸš€ Main tutorial function"""
    print("ğŸš€ LocoMuJoCo Tutorial 4: Learning to Walk")
    print("=" * 60)
    print("ğŸ¯ Understanding how robots learn through trial and error")
    
    # Core concept explanations
    explain_reinforcement_learning()
    demonstrate_reward_system() 
    
    # Learning process simulation
    simulate_learning_process()
    
    # Technical details
    demonstrate_real_training_setup()
    compare_learning_approaches()
    demonstrate_with_imitation_baseline()
    
    # Wrap up
    print(f"\nğŸ“ TUTORIAL COMPLETE - What You Just Learned:")
    print("=" * 60)
    print("âœ… Reinforcement learning = trial and error + rewards")
    print("âœ… Reward functions guide robot behavior like a teacher")
    print("âœ… Learning takes many attempts but finds novel solutions")
    print("âœ… Imitation learning is faster but less flexible")
    print("âœ… Real RL training would take hours/days of computation")
    
    print(f"\nğŸ’¡ CONCEPT: Real-World Applications")
    print("â”€" * 60)
    print("   ğŸ¤– Autonomous vehicles learning to drive")
    print("   ğŸ® Game AI mastering complex strategies")  
    print("   ğŸ¦¾ Prosthetic limbs adapting to users")
    print("   ğŸš Drones learning aerobatic maneuvers")
    print("   ğŸ­ Factory robots optimizing assembly")
    print("â”€" * 60)
    
    print(f"\nğŸ† EXPERIMENT TIME!")
    print("Try modifying this tutorial:")
    print("ğŸ’¡ Design your own reward function for different behaviors")
    print("ğŸ’¡ Think about what rewards would teach jumping vs dancing")
    print("ğŸ’¡ Consider multi-objective rewards (speed + efficiency)")
    print("ğŸ’¡ Explore how reward shaping affects learning speed")
    
    print(f"\nğŸ’¡ CONCEPT: What's Next?")
    print("â”€" * 60)  
    print("   Now you understand how robots learn! Advanced topics:")
    print("   â€¢ Policy gradient algorithms (PPO, SAC, etc.)")
    print("   â€¢ Sim-to-real transfer (simulation â†’ real robot)")
    print("   â€¢ Multi-agent learning (robots teaching each other)")
    print("   â€¢ Hierarchical RL (learning complex multi-step skills)")
    print("â”€" * 60)


if __name__ == "__main__":
    main()